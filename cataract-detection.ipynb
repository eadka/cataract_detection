{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5392218,"sourceType":"datasetVersion","datasetId":3115000}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Image to use for title\nimg_norm = Image.open('/kaggle/input/cataract-image-dataset/processed_images/test/normal/image_246.png')\nimg_norm\n\n\n### Load an Cataract image from the test dataset\nimg_cat = Image.open('/kaggle/input/cataract-image-dataset/processed_images/test/cataract/image_274.png')\nimg_cat.resize((530,350))\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom PIL import Image\nimport kagglehub\nimport torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:01:26.905551Z","iopub.execute_input":"2026-01-01T20:01:26.905923Z","iopub.status.idle":"2026-01-01T20:01:46.420815Z","shell.execute_reply.started":"2026-01-01T20:01:26.905889Z","shell.execute_reply":"2026-01-01T20:01:46.420193Z"}},"outputs":[{"name":"stderr","text":"2026-01-01 20:01:28.499811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767297688.650518      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767297688.694932      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767297689.050291      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767297689.050329      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767297689.050332      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767297689.050334      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install mplcyberpunk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:01:46.422026Z","iopub.execute_input":"2026-01-01T20:01:46.422484Z","iopub.status.idle":"2026-01-01T20:01:50.492390Z","shell.execute_reply.started":"2026-01-01T20:01:46.422459Z","shell.execute_reply":"2026-01-01T20:01:50.491666Z"}},"outputs":[{"name":"stdout","text":"Collecting mplcyberpunk\n  Downloading mplcyberpunk-0.7.6-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mplcyberpunk) (3.10.0)\nRequirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.12/dist-packages (from mplcyberpunk) (2.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mplcyberpunk) (1.3.3)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mplcyberpunk) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mplcyberpunk) (4.60.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mplcyberpunk) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mplcyberpunk) (25.0)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mplcyberpunk) (11.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mplcyberpunk) (3.2.5)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mplcyberpunk) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mplcyberpunk) (1.17.0)\nDownloading mplcyberpunk-0.7.6-py3-none-any.whl (6.5 kB)\nInstalling collected packages: mplcyberpunk\nSuccessfully installed mplcyberpunk-0.7.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport mplcyberpunk\nplt.style.use(\"cyberpunk\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:01:50.493542Z","iopub.execute_input":"2026-01-01T20:01:50.493801Z","iopub.status.idle":"2026-01-01T20:01:50.513901Z","shell.execute_reply.started":"2026-01-01T20:01:50.493773Z","shell.execute_reply":"2026-01-01T20:01:50.513156Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Download latest version\npath = kagglehub.dataset_download(\"nandanp6/cataract-image-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:01:50.515641Z","iopub.execute_input":"2026-01-01T20:01:50.515907Z","iopub.status.idle":"2026-01-01T20:01:50.678331Z","shell.execute_reply.started":"2026-01-01T20:01:50.515884Z","shell.execute_reply":"2026-01-01T20:01:50.677804Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/cataract-image-dataset\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Load the dataset","metadata":{}},{"cell_type":"code","source":"# Dataset paths\ntrain_dir = '/kaggle/input/cataract-image-dataset/processed_images/train/'\ntest_dir = '/kaggle/input/cataract-image-dataset/processed_images/test/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:01:50.679144Z","iopub.execute_input":"2026-01-01T20:01:50.679420Z","iopub.status.idle":"2026-01-01T20:01:50.682728Z","shell.execute_reply.started":"2026-01-01T20:01:50.679388Z","shell.execute_reply":"2026-01-01T20:01:50.682098Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Class names from the subdirectories in the training directory\nclass_names = sorted(os.listdir(train_dir))\n\n# Print the class names and the number of classes\nprint(\"Class Names: \", class_names)\nnum_classes = len(class_names)\nprint(\"Number of classes: \", num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T05:37:26.509320Z","iopub.execute_input":"2026-01-01T05:37:26.509504Z","iopub.status.idle":"2026-01-01T05:37:26.536648Z","shell.execute_reply.started":"2026-01-01T05:37:26.509484Z","shell.execute_reply":"2026-01-01T05:37:26.536134Z"}},"outputs":[{"name":"stdout","text":"Class Names:  ['cataract', 'normal']\nNumber of classes:  2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Visualization","metadata":{"execution":{"iopub.status.busy":"2025-12-26T21:22:18.169784Z","iopub.execute_input":"2025-12-26T21:22:18.170551Z","iopub.status.idle":"2025-12-26T21:22:18.173607Z","shell.execute_reply.started":"2025-12-26T21:22:18.170520Z","shell.execute_reply":"2025-12-26T21:22:18.173049Z"}}},{"cell_type":"code","source":"def visualize_images(path, target_size=(256, 256), num_images=5):\n\n    # Get a list of image filenames\n    image_filenames = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n    if not image_filenames:\n        raise ValueError(\"No images found in the specified path\")\n\n    # Select random images\n    selected_images = random.sample(image_filenames, min(num_images, len(image_filenames)))\n\n    # Create a figure and axes\n    fig, axes = plt.subplots(1, num_images, figsize=(15, 3), facecolor='black')\n\n    # Display each image\n    for i, image_filename in enumerate(selected_images):\n        # Load image and resize\n        image_path = os.path.join(path, image_filename)\n        image = Image.open(image_path)\n        image = image.resize(target_size)\n\n        # Display image\n        axes[i].imshow(image)\n        axes[i].axis('off')\n        axes[i].set_title(image_filename)  # Set image filename as title\n\n    # Adjust layout and display\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:42:41.996524Z","iopub.execute_input":"2025-12-28T19:42:41.996749Z","iopub.status.idle":"2025-12-28T19:42:42.011184Z","shell.execute_reply.started":"2025-12-28T19:42:41.996728Z","shell.execute_reply":"2025-12-28T19:42:42.010563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize Normal Eyes","metadata":{"execution":{"iopub.status.busy":"2025-12-26T21:22:30.567729Z","iopub.execute_input":"2025-12-26T21:22:30.568222Z","iopub.status.idle":"2025-12-26T21:22:30.571575Z","shell.execute_reply.started":"2025-12-26T21:22:30.568194Z","shell.execute_reply":"2025-12-26T21:22:30.570828Z"}}},{"cell_type":"code","source":"# Path name to visualize\nvisualize_path = '/kaggle/input/cataract-image-dataset/processed_images/train/normal/'\n\n# Visualize random images\nvisualize_images(visualize_path, num_images=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T05:24:51.228799Z","iopub.execute_input":"2025-12-28T05:24:51.229316Z","iopub.status.idle":"2025-12-28T05:24:52.262068Z","shell.execute_reply.started":"2025-12-28T05:24:51.229291Z","shell.execute_reply":"2025-12-28T05:24:52.261288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize Cataract Eyes","metadata":{}},{"cell_type":"code","source":"# Path name to visualize\nvisualize_path = '/kaggle/input/cataract-image-dataset/processed_images/train/cataract/'\n\n# Visualize random images\nvisualize_images(visualize_path, num_images=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T05:25:38.438123Z","iopub.execute_input":"2025-12-28T05:25:38.438421Z","iopub.status.idle":"2025-12-28T05:25:38.894618Z","shell.execute_reply.started":"2025-12-28T05:25:38.438396Z","shell.execute_reply":"2025-12-28T05:25:38.893951Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download a Pretrained Model - MobileNet V2\n\nMobileNet V2 is a lightweight convolutional neural network (CNN) architecture, specifically designed for mobile and embedded vision applications. Google researchers developed it as an enhancement over the original MobileNet model. Another remarkable aspect of this model is its ability to strike a good balance between model size and accuracy, rendering it ideal for resource-constrained devices. \nUseful reading here: https://www.analyticsvidhya.com/blog/2023/12/what-is-mobilenetv2/","metadata":{}},{"cell_type":"code","source":"# Load a pretrained model\nmodel = models.mobilenet_v2(weights='IMAGENET1K_V1')\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:01:50.683517Z","iopub.execute_input":"2026-01-01T20:01:50.683820Z","iopub.status.idle":"2026-01-01T20:01:51.009921Z","shell.execute_reply.started":"2026-01-01T20:01:50.683785Z","shell.execute_reply":"2026-01-01T20:01:51.009227Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.6M/13.6M [00:00<00:00, 113MB/s] \n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"MobileNetV2(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n    (1): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (9): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (10): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (11): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (12): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (13): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (14): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (15): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (16): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (17): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (18): Conv2dNormActivation(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Let's link the images to labels","metadata":{}},{"cell_type":"code","source":"# # Preprocessing for MobileNetV2\n# preprocess = transforms.Compose([\n#     transforms.Resize(256),\n#     transforms.CenterCrop(224),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n# ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:42:42.326333Z","iopub.execute_input":"2025-12-28T19:42:42.326633Z","iopub.status.idle":"2025-12-28T19:42:42.330746Z","shell.execute_reply.started":"2025-12-28T19:42:42.326599Z","shell.execute_reply":"2025-12-28T19:42:42.330008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CataractDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        self.classes = sorted(os.listdir(data_dir))\n        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n\n        for label_name in self.classes:\n            label_dir = os.path.join(data_dir, label_name)\n            for img_name in os.listdir(label_dir):\n                self.image_paths.append(os.path.join(label_dir, img_name))\n                self.labels.append(self.class_to_idx[label_name])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T04:06:16.472032Z","iopub.execute_input":"2025-12-29T04:06:16.472273Z","iopub.status.idle":"2025-12-29T04:06:16.478750Z","shell.execute_reply.started":"2025-12-29T04:06:16.472251Z","shell.execute_reply":"2025-12-29T04:06:16.478031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_size = 224\n\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Simple transforms - just resize and normalize\ntrain_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:43:17.566675Z","iopub.execute_input":"2025-12-28T19:43:17.567437Z","iopub.status.idle":"2025-12-28T19:43:17.571938Z","shell.execute_reply.started":"2025-12-28T19:43:17.567404Z","shell.execute_reply":"2025-12-28T19:43:17.571247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_dataset = CataractDataset(\n    data_dir= train_dir,\n    transform=train_transforms\n)\n\ntest_dataset = CataractDataset(\n    data_dir=test_dir,\n    transform=test_transforms\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:43:21.995522Z","iopub.execute_input":"2025-12-28T19:43:21.996059Z","iopub.status.idle":"2025-12-28T19:43:22.061737Z","shell.execute_reply.started":"2025-12-28T19:43:21.996026Z","shell.execute_reply":"2025-12-28T19:43:22.061048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting the full dataset indices into train and validation","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import random_split\n\nval_ratio = 0.2\ntotal_size = len(full_dataset)\nval_size = int(total_size * val_ratio)\ntrain_size = total_size - val_size\n\ntrain_dataset, val_dataset = random_split(\n    full_dataset,\n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:43:25.991693Z","iopub.execute_input":"2025-12-28T19:43:25.992313Z","iopub.status.idle":"2025-12-28T19:43:25.998482Z","shell.execute_reply.started":"2025-12-28T19:43:25.992281Z","shell.execute_reply":"2025-12-28T19:43:25.997707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:43:29.048180Z","iopub.execute_input":"2025-12-28T19:43:29.048482Z","iopub.status.idle":"2025-12-28T19:43:29.053162Z","shell.execute_reply.started":"2025-12-28T19:43:29.048456Z","shell.execute_reply":"2025-12-28T19:43:29.052570Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Base model","metadata":{}},{"cell_type":"code","source":"class CataractDetectorMobileNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(CataractDetectorMobileNet, self).__init__()\n        \n        # Load pre-trained MobileNetV2\n        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n        \n        # Freeze base model parameters\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        \n        # Remove original classifier\n        self.base_model.classifier = nn.Identity()\n        \n        # Add custom layers\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.output_layer = nn.Linear(1280, num_classes)\n\n    def forward(self, x):\n        x = self.base_model.features(x)\n        x = self.global_avg_pooling(x)\n        x = torch.flatten(x, 1)\n        x = self.output_layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:43:32.523847Z","iopub.execute_input":"2025-12-28T19:43:32.524532Z","iopub.status.idle":"2025-12-28T19:43:32.529748Z","shell.execute_reply.started":"2025-12-28T19:43:32.524504Z","shell.execute_reply":"2025-12-28T19:43:32.528944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CataractDetectorMobileNet(num_classes=2)\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:43:36.361339Z","iopub.execute_input":"2025-12-28T19:43:36.362069Z","iopub.status.idle":"2025-12-28T19:43:36.730720Z","shell.execute_reply.started":"2025-12-28T19:43:36.362037Z","shell.execute_reply":"2025-12-28T19:43:36.729971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Function to train and evaluate the model","metadata":{"execution":{"iopub.status.busy":"2025-12-28T05:35:38.994219Z","iopub.execute_input":"2025-12-28T05:35:38.994529Z","iopub.status.idle":"2025-12-28T05:35:38.998264Z","shell.execute_reply.started":"2025-12-28T05:35:38.994498Z","shell.execute_reply":"2025-12-28T05:35:38.997573Z"}}},{"cell_type":"code","source":"def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T05:34:30.143621Z","iopub.execute_input":"2025-12-28T05:34:30.144395Z","iopub.status.idle":"2025-12-28T05:34:30.151311Z","shell.execute_reply.started":"2025-12-28T05:34:30.144365Z","shell.execute_reply":"2025-12-28T05:34:30.150749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_model(learning_rate=0.01):\n    model = CataractDetectorMobileNet(num_classes=2)\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    return model, optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T05:34:44.939205Z","iopub.execute_input":"2025-12-28T05:34:44.939857Z","iopub.status.idle":"2025-12-28T05:34:44.943757Z","shell.execute_reply.started":"2025-12-28T05:34:44.939828Z","shell.execute_reply":"2025-12-28T05:34:44.943069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 10\nmodel, optimizer = make_model(learning_rate=0.01)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T05:28:49.257231Z","iopub.execute_input":"2025-12-28T05:28:49.257787Z","iopub.status.idle":"2025-12-28T05:31:24.756617Z","shell.execute_reply.started":"2025-12-28T05:28:49.257761Z","shell.execute_reply":"2025-12-28T05:31:24.755775Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model achieved an average training accuracy of 90.28% and an average validation accuracy of 95.21% over 10 epochs. Validation accuracy consistently exceeded training accuracy due to the use of data augmentation during training, which improved generalisation.","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter tuning\n\nHyperparameter tuning is performed using a sequential, experiment-driven approach rather than exhaustive grid search due to the high computational cost and non-convex nature of deep neural networks. Parameters were tuned individually to understand their impact on validation performance and generalization. We tune the following hyperparameters below:\n\n- Learning rate\n- Inner size\n- Dropout\n- Augmentation\n- Early stopping","metadata":{}},{"cell_type":"markdown","source":"### Adjusting the learning rate","metadata":{}},{"cell_type":"code","source":"learning_rates = [0.0001, 0.001, 0.01, 0.1]\n\nfor lr in learning_rates:\n    print(f'\\n=== Learning Rate: {lr} ===')\n    model, optimizer = make_model(learning_rate=lr)\n    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T21:50:15.413078Z","iopub.execute_input":"2025-12-27T21:50:15.413428Z","iopub.status.idle":"2025-12-27T22:00:25.557246Z","shell.execute_reply.started":"2025-12-27T21:50:15.413400Z","shell.execute_reply":"2025-12-27T22:00:25.556433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A learning rate sweep was conducted across {0.0001, 0.001, 0.01, 0.1}.\n\nA learning rate of 0.001 demonstrated the most stable convergence and consistent validation performance, achieving validation accuracy up to 97.96% without instability.\n\nHigher learning rates (0.01, 0.1) showed faster initial convergence but resulted in training instability and performance degradation in later epochs, while lower learning rates (0.0001) converged too slowly.","metadata":{}},{"cell_type":"markdown","source":"### Learning Rate Comparison\n\n| Learning Rate | Peak Validation Accuracy | Stability | Recommendation |\n|---------------|--------------------------|-----------|----------------|\n| 0.0001        | ~94.9%                   | High      | ❌ Too slow    |\n| **0.001**     | **97.96%**               | **High**  | ✅ **Best choice** |\n| 0.01          | ~98.0%                   | Low       | ⚠️ Risky      |\n| 0.1           | ~97.9%                   | Very low  | ❌ Not recommended |","metadata":{"execution":{"iopub.status.busy":"2025-12-27T22:12:01.612981Z","iopub.execute_input":"2025-12-27T22:12:01.613832Z","iopub.status.idle":"2025-12-27T22:12:01.619476Z","shell.execute_reply.started":"2025-12-27T22:12:01.613799Z","shell.execute_reply":"2025-12-27T22:12:01.618631Z"}}},{"cell_type":"markdown","source":"### Adding an inner layer ","metadata":{}},{"cell_type":"code","source":"class CataractDetectorMobileNet(nn.Module):\n    def __init__(self, size_inner=100, num_classes=2):\n        super(CataractDetectorMobileNet, self).__init__()\n        \n        # Load pre-trained MobileNetV2\n        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n        \n        # Freeze base model parameters\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        \n        # Remove original classifier\n        self.base_model.classifier = nn.Identity()\n        \n        # Add custom layers\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n        self.relu = nn.ReLU()\n        self.output_layer = nn.Linear(size_inner, num_classes)\n\n    def forward(self, x):\n        x = self.base_model.features(x)\n        x = self.global_avg_pooling(x)\n        x = torch.flatten(x, 1)\n        x = self.inner(x)\n        x = self.relu(x)\n        x = self.output_layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T05:45:35.292037Z","iopub.execute_input":"2025-12-28T05:45:35.292332Z","iopub.status.idle":"2025-12-28T05:45:35.298249Z","shell.execute_reply.started":"2025-12-28T05:45:35.292308Z","shell.execute_reply":"2025-12-28T05:45:35.297581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_model(learning_rate=0.001, size_inner=100):\n    model = CataractDetectorMobileNet(\n        num_classes=2,\n        size_inner=size_inner\n    )\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    return model, optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:44:13.122059Z","iopub.execute_input":"2025-12-28T19:44:13.122719Z","iopub.status.idle":"2025-12-28T19:44:13.126616Z","shell.execute_reply.started":"2025-12-28T19:44:13.122686Z","shell.execute_reply":"2025-12-28T19:44:13.126032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"size_inner = [1000, 500, 100]\n\nfor s in size_inner:\n    print(f'\\n=== Size Inner: {s} ===')\n    model, optimizer = make_model(learning_rate=0.001, size_inner=s)\n    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T05:45:39.040546Z","iopub.execute_input":"2025-12-28T05:45:39.041101Z","iopub.status.idle":"2025-12-28T05:53:12.228431Z","shell.execute_reply.started":"2025-12-28T05:45:39.041076Z","shell.execute_reply":"2025-12-28T05:53:12.227763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experimentation with different classifier head capacities on top of a frozen MobileNetV2 backbone. An intermediate hidden dimension of 500 achieved the best tradeoff between model expressiveness and generalization, yielding the highest and most stable validation accuracy (~99%). Smaller heads underfit, while larger heads showed early signs of overfitting.","metadata":{}},{"cell_type":"markdown","source":"### Classifier Head Capacity Comparison\n\n| Inner Layer Size | Peak Validation Accuracy | Stability | Interpretation |\n|------------------|--------------------------|-----------|----------------|\n| **1000**         | ~96.94%                  | High      | Slight overcapacity |\n| **500**          | **98.98%**               | **Best**  | ✅ Sweet spot – best bias–variance tradeoff |\n| **100**          | ~96.94%                  | Lower     | Under-capacity / mild underfitting |","metadata":{}},{"cell_type":"markdown","source":"### Conclusion\n\nThe results indicate that the capacity of the classifier head has a significant impact on model performance and stability. A smaller inner layer size (100) shows signs of underfitting, leading to less consistent validation performance. Conversely, a larger inner layer size (1000) introduces increased model capacity, which slightly improves training accuracy but shows early signs of overfitting.\n\nAn intermediate inner layer size of **500** achieves the best balance between expressiveness and generalization, delivering the highest and most stable validation accuracy (~99%). This configuration was selected as the final architecture for the cataract detection model.\n","metadata":{"execution":{"iopub.status.busy":"2025-12-28T05:59:27.054435Z","iopub.execute_input":"2025-12-28T05:59:27.055071Z","iopub.status.idle":"2025-12-28T05:59:27.060587Z","shell.execute_reply.started":"2025-12-28T05:59:27.055045Z","shell.execute_reply":"2025-12-28T05:59:27.059661Z"}}},{"cell_type":"markdown","source":"### Adding checkpointing","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\n\n        # Checkpoint the model if validation accuracy improved\n        if val_acc > best_val_accuracy:\n            best_val_accuracy = val_acc\n            checkpoint_path = f'mobilenet_v2_{epoch+1:02d}_{val_acc:.3f}.pth'\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f'Checkpoint saved: {checkpoint_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:44:00.881169Z","iopub.execute_input":"2025-12-28T19:44:00.881483Z","iopub.status.idle":"2025-12-28T19:44:00.890112Z","shell.execute_reply.started":"2025-12-28T19:44:00.881456Z","shell.execute_reply":"2025-12-28T19:44:00.889436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CataractDetectorMobileNet(nn.Module):\n    def __init__(self, size_inner=100, num_classes=2):\n        super(CataractDetectorMobileNet, self).__init__()\n        \n        # Load pre-trained MobileNetV2\n        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n        \n        # Freeze base model parameters\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        \n        # Remove original classifier\n        self.base_model.classifier = nn.Identity()\n        \n        # Add custom layers\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n        self.relu = nn.ReLU()\n        self.output_layer = nn.Linear(size_inner, num_classes)\n\n    def forward(self, x):\n        x = self.base_model.features(x)\n        x = self.global_avg_pooling(x)\n        x = torch.flatten(x, 1)\n        x = self.inner(x)\n        x = self.relu(x)\n        x = self.output_layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:45:17.448155Z","iopub.execute_input":"2025-12-28T19:45:17.448969Z","iopub.status.idle":"2025-12-28T19:45:17.455000Z","shell.execute_reply.started":"2025-12-28T19:45:17.448927Z","shell.execute_reply":"2025-12-28T19:45:17.454311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_model(learning_rate=0.001, size_inner=100):\n    model = CataractDetectorMobileNet(\n        num_classes=2,\n        size_inner=size_inner\n    )\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    return model, optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:45:28.227267Z","iopub.execute_input":"2025-12-28T19:45:28.227879Z","iopub.status.idle":"2025-12-28T19:45:28.231902Z","shell.execute_reply.started":"2025-12-28T19:45:28.227851Z","shell.execute_reply":"2025-12-28T19:45:28.231175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"size_inner = 500\nlearning_rate = 0.001\nnum_epochs = 10\n\nmodel, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T19:45:44.762987Z","iopub.execute_input":"2025-12-28T19:45:44.763693Z","iopub.status.idle":"2025-12-28T19:48:20.775856Z","shell.execute_reply.started":"2025-12-28T19:45:44.763663Z","shell.execute_reply":"2025-12-28T19:48:20.775170Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Checkpointing\n\nTo ensure that the best-performing model is preserved during training, **model checkpointing** was implemented based on validation accuracy.\n\nAfter each training epoch, the model is evaluated on the validation dataset. If the validation accuracy improves compared to previous epochs, the current model weights are saved to disk. This guarantees that the final selected model represents the best generalization performance, rather than simply the final training epoch.\n\nEach checkpoint filename encodes the epoch number and validation accuracy for traceability:\n\n``` mobilenet_v2_<epoch>_<val_accuracy>.pth```","metadata":{}},{"cell_type":"markdown","source":"### Checkpointing Criteria\n- **Metric monitored:** Validation Accuracy\n- **Save condition:** `val_accuracy > best_val_accuracy`\n- **Model format:** PyTorch `state_dict`\n\n### Observations\n- The model achieved its best validation accuracy (**96.94%**) early in training (Epoch 2).\n- Subsequent epochs showed higher training accuracy but fluctuating validation performance, highlighting the importance of checkpointing to avoid overfitting.\n- The checkpointing strategy ensures reproducibility and reliable model selection for deployment.\n\nThis approach ensures that the deployed model reflects optimal validation performance rather than training convergence alone.\n","metadata":{}},{"cell_type":"markdown","source":"### Dropout","metadata":{}},{"cell_type":"code","source":"class CataractDetectorMobileNet(nn.Module):\n    def __init__(self, size_inner=500, droprate=0.2, num_classes=2):\n        super(CataractDetectorMobileNet, self).__init__()\n        \n        # Load pre-trained MobileNetV2\n        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n        \n        # Freeze base model parameters\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        \n        # Remove original classifier\n        self.base_model.classifier = nn.Identity()\n        \n        # Add custom layers\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(droprate)  # Add dropout\n        self.output_layer = nn.Linear(size_inner, num_classes)\n\n    def forward(self, x):\n        x = self.base_model.features(x)\n        x = self.global_avg_pooling(x)\n        x = torch.flatten(x, 1)\n        x = self.inner(x)\n        x = self.relu(x)\n        x = self.dropout(x)  # Apply dropout\n        x = self.output_layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T20:01:52.744113Z","iopub.execute_input":"2025-12-28T20:01:52.744672Z","iopub.status.idle":"2025-12-28T20:01:52.750629Z","shell.execute_reply.started":"2025-12-28T20:01:52.744638Z","shell.execute_reply":"2025-12-28T20:01:52.750021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_model(learning_rate=0.001, size_inner=500, droprate=0.2):\n    model = CataractDetectorMobileNet(\n        num_classes=2,\n        size_inner=size_inner,\n        droprate=droprate\n    )\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    return model, optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T20:02:18.923186Z","iopub.execute_input":"2025-12-28T20:02:18.923757Z","iopub.status.idle":"2025-12-28T20:02:18.927986Z","shell.execute_reply.started":"2025-12-28T20:02:18.923725Z","shell.execute_reply":"2025-12-28T20:02:18.927289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Changing the version in checkpointing to V3\n\n``` mobilenet_v3_<epoch>_<val_accuracy>.pth```","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\n\n        # Checkpoint the model if validation accuracy improved\n        if val_acc > best_val_accuracy:\n            best_val_accuracy = val_acc\n            checkpoint_path = f'mobilenet_v3_{epoch+1:02d}_{val_acc:.3f}.pth'\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f'Checkpoint saved: {checkpoint_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T20:03:41.720817Z","iopub.execute_input":"2025-12-28T20:03:41.721569Z","iopub.status.idle":"2025-12-28T20:03:41.729441Z","shell.execute_reply.started":"2025-12-28T20:03:41.721538Z","shell.execute_reply":"2025-12-28T20:03:41.728662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"size_inner = 500\nlearning_rate = 0.001\nnum_epochs = 50 # We increase the number of size of the epochs because needs a longer time to learn\n\n\nfor droprate in [0.1,0.2,0.5,0.7]:\n    print(f'\\n=== Droprate: {droprate} ===')\n    model, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner,droprate=droprate)\n    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T20:04:14.956492Z","iopub.execute_input":"2025-12-28T20:04:14.957387Z","iopub.status.idle":"2025-12-28T20:54:23.899833Z","shell.execute_reply.started":"2025-12-28T20:04:14.957354Z","shell.execute_reply":"2025-12-28T20:54:23.899004Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dropout Regularisation Experiments\n\nTo reduce overfitting and improve generalisation, **dropout** was introduced after the inner fully connected layer of the MobileNetV2-based cataract classifier. Experiments were conducted with multiple dropout rates while keeping the inner layer size fixed at **500 units**.\n\nAll models were trained for **50 epochs** with validation-based checkpointing enabled.\n\n### Experimental Setup\n- Backbone: Pre-trained **MobileNetV2** (frozen)\n- Inner layer size: `500`\n- Optimiser: Adam (`lr = 0.001`)\n- Epochs: `50`\n- Metric monitored: **Validation Accuracy**\n- Dropout applied before output layer\n\n---\n\n### Summary of Results\n\n| Dropout Rate | Best Val Accuracy | Training Behaviour | Validation Stability | Notes |\n|-------------|------------------|-------------------|----------------------|------|\n| **0.1** | **96.94%** | Very fast convergence, near-perfect train acc | High variance after early epochs | Signs of overfitting |\n| **0.2** | **98.98%** | Strong learning with controlled train acc | Most stable overall | **Best performer** |\n| **0.5** | **96.94%** | Slower learning, slightly noisier | Stable but lower ceiling | Mild underfitting |\n| **0.7** | **97.96%** | Slower convergence | Consistent but conservative | Over-regularisation |\n\n---\n\n### Key Observations\n\n- **Dropout = 0.1**\n  - Achieves high training accuracy very quickly.\n  - Validation accuracy peaks early but fluctuates significantly later.\n  - Indicates **overfitting**, despite high raw accuracy.\n\n- **Dropout = 0.2**\n  - Achieves the **highest validation accuracy (98.98%)**.\n  - Maintains a strong balance between training performance and validation stability.\n  - Produces the **most reliable checkpoints** across epochs.\n\n- **Dropout = 0.5**\n  - Introduces stronger regularisation.\n  - Validation accuracy is stable but slightly lower than optimal.\n  - Suitable when dataset size is very small.\n\n- **Dropout = 0.7**\n  - Heavily regularised model.\n  - Slower learning and reduced peak performance.\n  - Suggests **over-regularisation**, limiting representational capacity.\n\n---\n\n### Conclusion\n\nA dropout rate of **0.2** provides the best trade-off between model capacity and regularisation. It consistently delivers the highest validation accuracy with reduced variance and improved generalisation, making it the **recommended configuration for deployment**.\n\nThis experiment highlights the importance of tuning regularisation strength, especially when fine-tuning compact CNN architectures on limited medical imaging datasets.\n","metadata":{}},{"cell_type":"markdown","source":"## Data Augmentations and Early stopping","metadata":{}},{"cell_type":"markdown","source":"### Data Augmentation\nWe now augment the dataset to make it richer and more diverse to improve the prediction accuracy. \n\nData augmentation strategies were designed to reflect realistic ophthalmic imaging variations, including mild rotation, illumination changes, and blur. These transformations improve robustness while preserving clinically relevant cataract features. Validation and test sets were kept deterministic to ensure unbiased evaluation.\n\nThe data augmentation strategies were introduced incrementally. Rotation was applied first, followed by blur to simulate cataract-induced opacity, and brightness/contrast variation to reflect illumination changes. Each addition was validated independently before composing the final augmentation pipeline.\n\n### Early Stopping\nEarly stopping is used to prevented overfitting and unnecessary training once validation performance plateaus.","metadata":{}},{"cell_type":"markdown","source":"#### RandomRotation(10): Rotate the input by 10 Degrees.","metadata":{}},{"cell_type":"code","source":"input_size = 224\n\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Training transforms WITH augmentation \ntrain_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n\n    transforms.RandomRotation(10),\n\n    # transforms.GaussianBlur(\n    # kernel_size=3,\n    # sigma=(0.1, 1.0)\n    # ),\n    \n    # transforms.ColorJitter(\n    #     brightness=0.15,\n    #     contrast=0.15\n    # ),\n\n    # transforms.RandomAffine(\n    # degrees=5,\n    # translate=(0.02, 0.02),\n    # scale=(0.98, 1.02)\n    # ),\n\n    # transforms.RandomHorizontalFlip(p=0.5),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:02:41.636915Z","iopub.execute_input":"2026-01-01T20:02:41.637221Z","iopub.status.idle":"2026-01-01T20:02:41.643135Z","shell.execute_reply.started":"2026-01-01T20:02:41.637195Z","shell.execute_reply":"2026-01-01T20:02:41.642402Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class CataractDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        self.classes = sorted(os.listdir(data_dir))\n        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n\n        for label_name in self.classes:\n            label_dir = os.path.join(data_dir, label_name)\n            for img_name in os.listdir(label_dir):\n                self.image_paths.append(os.path.join(label_dir, img_name))\n                self.labels.append(self.class_to_idx[label_name])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:02:45.251448Z","iopub.execute_input":"2026-01-01T20:02:45.252187Z","iopub.status.idle":"2026-01-01T20:02:45.258096Z","shell.execute_reply.started":"2026-01-01T20:02:45.252154Z","shell.execute_reply":"2026-01-01T20:02:45.257385Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dataset = CataractDataset(\n    data_dir=train_dir,\n    transform=train_transforms\n)\n\nval_dataset = CataractDataset(\n    data_dir=train_dir,\n    transform=val_transforms\n)\n\ntest_dataset = CataractDataset(\n    data_dir=test_dir,\n    transform=test_transforms\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:02:48.357876Z","iopub.execute_input":"2026-01-01T20:02:48.358391Z","iopub.status.idle":"2026-01-01T20:02:48.432722Z","shell.execute_reply.started":"2026-01-01T20:02:48.358331Z","shell.execute_reply":"2026-01-01T20:02:48.432017Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"indices = list(range(len(train_dataset)))\ntrain_indices, val_indices = train_test_split(\n    indices, test_size=0.2, random_state=42\n)\n\ntrain_dataset = Subset(train_dataset, train_indices)\nval_dataset = Subset(val_dataset, val_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:02:50.785158Z","iopub.execute_input":"2026-01-01T20:02:50.785738Z","iopub.status.idle":"2026-01-01T20:02:50.790816Z","shell.execute_reply.started":"2026-01-01T20:02:50.785704Z","shell.execute_reply":"2026-01-01T20:02:50.790174Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:02:51.137549Z","iopub.execute_input":"2026-01-01T20:02:51.138189Z","iopub.status.idle":"2026-01-01T20:02:51.142010Z","shell.execute_reply.started":"2026-01-01T20:02:51.138162Z","shell.execute_reply":"2026-01-01T20:02:51.141420Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device, patience=5):\n    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n    epochs_without_improvement = 0\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\n\n        # Checkpoint the model if validation accuracy improved\n        if val_acc > best_val_accuracy:\n            best_val_accuracy = val_acc\n            epochs_without_improvement = 0\n\n            \n            checkpoint_path = f'mobilenet_v4_{epoch+1:02d}_{val_acc:.3f}.pth' # Changing version to 4 for data augmentations\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f'Checkpoint saved: {checkpoint_path}')\n\n        else:\n            epochs_without_improvement += 1\n            print(f'⚠️ No improvement for {epochs_without_improvement} epoch(s)')\n        \n            if epochs_without_improvement >= patience:\n                print(f'🛑 Early stopping triggered after {epoch+1} epochs')\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:02:53.944875Z","iopub.execute_input":"2026-01-01T20:02:53.945468Z","iopub.status.idle":"2026-01-01T20:02:53.954047Z","shell.execute_reply.started":"2026-01-01T20:02:53.945437Z","shell.execute_reply":"2026-01-01T20:02:53.953307Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def make_model(learning_rate=0.001, size_inner=500, droprate=0.2):\n    model = CataractDetectorMobileNet(\n        num_classes=2,\n        size_inner=size_inner,\n        droprate=droprate\n    )\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    return model, optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:02:55.779810Z","iopub.execute_input":"2026-01-01T20:02:55.780516Z","iopub.status.idle":"2026-01-01T20:02:55.784244Z","shell.execute_reply.started":"2026-01-01T20:02:55.780487Z","shell.execute_reply":"2026-01-01T20:02:55.783619Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class CataractDetectorMobileNet(nn.Module):\n    def __init__(self, size_inner=500, droprate=0.2, num_classes=2):\n        super(CataractDetectorMobileNet, self).__init__()\n        \n        # Load pre-trained MobileNetV2\n        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n        \n        # Freeze base model parameters\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        \n        # Remove original classifier\n        self.base_model.classifier = nn.Identity()\n        \n        # Add custom layers\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(droprate)  # Add dropout\n        self.output_layer = nn.Linear(size_inner, num_classes)\n\n    def forward(self, x):\n        x = self.base_model.features(x)\n        x = self.global_avg_pooling(x)\n        x = torch.flatten(x, 1)\n        x = self.inner(x)\n        x = self.relu(x)\n        x = self.dropout(x)  # Apply dropout\n        x = self.output_layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:02:56.988899Z","iopub.execute_input":"2026-01-01T20:02:56.989378Z","iopub.status.idle":"2026-01-01T20:02:56.994910Z","shell.execute_reply.started":"2026-01-01T20:02:56.989337Z","shell.execute_reply":"2026-01-01T20:02:56.994277Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CataractDetectorMobileNet(num_classes=2)\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:03:00.303061Z","iopub.execute_input":"2026-01-01T20:03:00.303760Z","iopub.status.idle":"2026-01-01T20:03:00.676443Z","shell.execute_reply.started":"2026-01-01T20:03:00.303718Z","shell.execute_reply":"2026-01-01T20:03:00.675864Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"droprate = 0.2\nsize_inner = 500\nlearning_rate = 0.001\nnum_epochs = 25\n\n\nprint(f'\\n=== With Data Augmentation: ===')\nmodel, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner,droprate=droprate)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T20:08:59.577222Z","iopub.execute_input":"2025-12-29T20:08:59.577880Z","iopub.status.idle":"2025-12-29T20:12:22.723647Z","shell.execute_reply.started":"2025-12-29T20:08:59.577837Z","shell.execute_reply":"2025-12-29T20:12:22.722877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### GaussianBlur: Blurs image with randomly chosen Gaussian blur kernel.","metadata":{"execution":{"iopub.status.busy":"2025-12-29T20:24:40.483707Z","iopub.execute_input":"2025-12-29T20:24:40.484454Z","iopub.status.idle":"2025-12-29T20:24:40.488215Z","shell.execute_reply.started":"2025-12-29T20:24:40.484424Z","shell.execute_reply":"2025-12-29T20:24:40.487648Z"}}},{"cell_type":"code","source":"input_size = 224\n\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Training transforms WITH augmentation \ntrain_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n\n    transforms.RandomRotation(10),\n\n    transforms.GaussianBlur(\n    kernel_size=3,\n    sigma=(0.1, 1.0)\n    ),\n    \n    # transforms.ColorJitter(\n    #     brightness=0.1,\n    #     contrast=0.1\n    # ),\n\n    # transforms.RandomAffine(\n    # degrees=3,\n    # translate=(0.01, 0.01),\n    # scale=(0.99, 1.01)\n    # ),\n\n    # transforms.RandomHorizontalFlip(p=0.5),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:03:05.699155Z","iopub.execute_input":"2026-01-01T20:03:05.700016Z","iopub.status.idle":"2026-01-01T20:03:05.705862Z","shell.execute_reply.started":"2026-01-01T20:03:05.699963Z","shell.execute_reply":"2026-01-01T20:03:05.705152Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_dataset = CataractDataset(\n    data_dir=train_dir,\n    transform=train_transforms\n)\n\nval_dataset = CataractDataset(\n    data_dir=train_dir,\n    transform=val_transforms\n)\n\ntest_dataset = CataractDataset(\n    data_dir=test_dir,\n    transform=test_transforms\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T20:29:00.185677Z","iopub.execute_input":"2025-12-29T20:29:00.186104Z","iopub.status.idle":"2025-12-29T20:29:00.221151Z","shell.execute_reply.started":"2025-12-29T20:29:00.186064Z","shell.execute_reply":"2025-12-29T20:29:00.220349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"indices = list(range(len(train_dataset)))\ntrain_indices, val_indices = train_test_split(\n    indices, test_size=0.2, random_state=42\n)\n\ntrain_dataset = Subset(train_dataset, train_indices)\nval_dataset = Subset(val_dataset, val_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T20:29:39.046955Z","iopub.execute_input":"2025-12-29T20:29:39.047264Z","iopub.status.idle":"2025-12-29T20:29:39.052676Z","shell.execute_reply.started":"2025-12-29T20:29:39.047239Z","shell.execute_reply":"2025-12-29T20:29:39.052034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T20:30:04.254974Z","iopub.execute_input":"2025-12-29T20:30:04.255575Z","iopub.status.idle":"2025-12-29T20:30:04.259539Z","shell.execute_reply.started":"2025-12-29T20:30:04.255547Z","shell.execute_reply":"2025-12-29T20:30:04.258832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"droprate = 0.2\nsize_inner = 500\nlearning_rate = 0.001\nnum_epochs = 25\n\n\nprint(f'\\n=== With Data Augmentation using RandomRotation and Gaussian Blur: ===')\nmodel, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner,droprate=droprate)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T20:30:33.004647Z","iopub.execute_input":"2025-12-29T20:30:33.005471Z","iopub.status.idle":"2025-12-29T20:34:00.156133Z","shell.execute_reply.started":"2025-12-29T20:30:33.005442Z","shell.execute_reply":"2025-12-29T20:34:00.155411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ⚠️ Observation:\nGaussian blur augmentation was evaluated to improve robustness to image quality variations. However, results showed a consistent drop in validation accuracy and increased instability compared to rotation-only augmentation. This suggests that blur removes diagnostically relevant features for cataract detection. Consequently, Gaussian blur was excluded from the final augmentation pip","metadata":{}},{"cell_type":"markdown","source":"### Unfreezing last MobileNet Block\nTrying the same augmentation by unfreezing the last MobileNet block","metadata":{}},{"cell_type":"code","source":"class CataractDetectorMobileNet(nn.Module):\n    def __init__(self, size_inner=500, droprate=0.2, num_classes=2):\n        super(CataractDetectorMobileNet, self).__init__()\n        \n        # Load pre-trained MobileNetV2\n        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n        \n        # # Freeze base model parameters\n        # for param in self.base_model.parameters():\n        #     param.requires_grad = False\n\n        for param in model.base_model.features[-1].parameters():\n            param.requires_grad = True\n        \n        # Remove original classifier\n        self.base_model.classifier = nn.Identity()\n        \n        # Add custom layers\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(droprate)  # Add dropout\n        self.output_layer = nn.Linear(size_inner, num_classes)\n\n    def forward(self, x):\n        x = self.base_model.features(x)\n        x = self.global_avg_pooling(x)\n        x = torch.flatten(x, 1)\n        x = self.inner(x)\n        x = self.relu(x)\n        x = self.dropout(x)  # Apply dropout\n        x = self.output_layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:03:13.125918Z","iopub.execute_input":"2026-01-01T20:03:13.126616Z","iopub.status.idle":"2026-01-01T20:03:13.132505Z","shell.execute_reply.started":"2026-01-01T20:03:13.126584Z","shell.execute_reply":"2026-01-01T20:03:13.131842Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"droprate = 0.2\nsize_inner = 500\nlearning_rate = 1e-4 # 0.001\nnum_epochs = 25\n\n\nprint(f'\\n=== With Data Augmentation using RandomRotation and Gaussian Blur: ===')\nmodel, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner,droprate=droprate)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T20:03:12.987417Z","iopub.execute_input":"2025-12-31T20:03:12.987777Z","iopub.status.idle":"2025-12-31T20:10:20.275180Z","shell.execute_reply.started":"2025-12-31T20:03:12.987748Z","shell.execute_reply":"2025-12-31T20:10:20.273550Z"}},"outputs":[{"name":"stdout","text":"\n=== With Data Augmentation using RandomRotation and Gaussian Blur: ===\nEpoch 1/25\n  Train Loss: 0.4664, Train Acc: 0.7551\n  Val Loss: 0.2833, Val Acc: 0.8687\nCheckpoint saved: mobilenet_v4_01_0.869.pth\nEpoch 2/25\n  Train Loss: 0.1386, Train Acc: 0.9541\n  Val Loss: 0.1384, Val Acc: 0.9394\nCheckpoint saved: mobilenet_v4_02_0.939.pth\nEpoch 3/25\n  Train Loss: 0.0831, Train Acc: 0.9770\n  Val Loss: 0.1008, Val Acc: 0.9596\nCheckpoint saved: mobilenet_v4_03_0.960.pth\nEpoch 4/25\n  Train Loss: 0.0347, Train Acc: 0.9898\n  Val Loss: 0.0867, Val Acc: 0.9798\nCheckpoint saved: mobilenet_v4_04_0.980.pth\nEpoch 5/25\n  Train Loss: 0.0306, Train Acc: 0.9949\n  Val Loss: 0.1189, Val Acc: 0.9596\n⚠️ No improvement for 1 epoch(s)\nEpoch 6/25\n  Train Loss: 0.0179, Train Acc: 0.9949\n  Val Loss: 0.1353, Val Acc: 0.9394\n⚠️ No improvement for 2 epoch(s)\nEpoch 7/25\n  Train Loss: 0.0267, Train Acc: 0.9923\n  Val Loss: 0.1380, Val Acc: 0.9495\n⚠️ No improvement for 3 epoch(s)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3482780474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n=== With Data Augmentation using RandomRotation and Gaussian Blur: ==='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_inner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdroprate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdroprate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/1695174927.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, optimizer, train_loader, val_loader, criterion, num_epochs, device, patience)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"### Observation:\nUnfreezing the final MobileNetV2 block significantly improved performance when combined with data augmentation, increasing validation accuracy from ~0.90 to ~0.96. This indicates that domain-specific adaptation is essential for handling augmentations such as Gaussian blur. However, a high learning rate led to rapid overfitting, motivating the use of lower or differential learning rates for fine-tuning. Hence the learning rate was changed from 0.001 to 1e-4.","metadata":{}},{"cell_type":"markdown","source":"## Data Augmentation with color jitter\n\nNow we try another data augmentation with color jitter along with the earlier augmentations of RandomRotation and GaussianBlur","metadata":{}},{"cell_type":"code","source":"input_size = 224\n\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Training transforms WITH augmentation \ntrain_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n\n    transforms.RandomRotation(10),\n\n    transforms.GaussianBlur(\n    kernel_size=3,\n    sigma=(0.1, 1.0)\n    ),\n    \n    transforms.ColorJitter(\n        brightness=0.1,\n        contrast=0.1\n    ),\n\n    # transforms.RandomAffine(\n    # degrees=3,\n    # translate=(0.01, 0.01),\n    # scale=(0.99, 1.01)\n    # ),\n\n    # transforms.RandomHorizontalFlip(p=0.5),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:03:18.637417Z","iopub.execute_input":"2026-01-01T20:03:18.638041Z","iopub.status.idle":"2026-01-01T20:03:18.644711Z","shell.execute_reply.started":"2026-01-01T20:03:18.638007Z","shell.execute_reply":"2026-01-01T20:03:18.643875Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def loader_func(train_dir, test_dir, train_transforms, val_transforms, test_transforms):\n    train_dataset = CataractDataset(\n    data_dir=train_dir,\n    transform=train_transforms\n    )\n    \n    val_dataset = CataractDataset(\n        data_dir=train_dir,\n        transform=val_transforms\n    )\n    \n    test_dataset = CataractDataset(\n        data_dir=test_dir,\n        transform=test_transforms\n    )\n    \n    indices = list(range(len(train_dataset)))\n    train_indices, val_indices = train_test_split(\n        indices, test_size=0.2, random_state=42\n    )\n    \n    train_dataset = Subset(train_dataset, train_indices)\n    val_dataset = Subset(val_dataset, val_indices)\n    \n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:03:21.621740Z","iopub.execute_input":"2026-01-01T20:03:21.622405Z","iopub.status.idle":"2026-01-01T20:03:21.627188Z","shell.execute_reply.started":"2026-01-01T20:03:21.622374Z","shell.execute_reply":"2026-01-01T20:03:21.626630Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"droprate = 0.2\nsize_inner = 500\nlearning_rate = 1e-4 # 0.001\nnum_epochs = 25\n\n\ntrain_loader, val_loader, test_loader = loader_func(train_dir, test_dir, train_transforms, val_transforms, test_transforms)\n\nprint(f'\\n=== With Data Augmentation using Random Rotation, Gaussian Blur and Color Jitter: ===')\nmodel, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner,droprate=droprate)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T20:20:58.122893Z","iopub.execute_input":"2025-12-31T20:20:58.123485Z","iopub.status.idle":"2025-12-31T20:25:03.003305Z","shell.execute_reply.started":"2025-12-31T20:20:58.123456Z","shell.execute_reply":"2025-12-31T20:25:03.002546Z"}},"outputs":[{"name":"stdout","text":"\n=== With Data Augmentation using Random Rotation, Gaussian Blur and Color Jitter: ===\nEpoch 1/25\n  Train Loss: 0.4032, Train Acc: 0.8291\n  Val Loss: 0.2064, Val Acc: 0.8687\nCheckpoint saved: mobilenet_v4_01_0.869.pth\nEpoch 2/25\n  Train Loss: 0.1952, Train Acc: 0.9515\n  Val Loss: 0.1193, Val Acc: 0.9495\nCheckpoint saved: mobilenet_v4_02_0.949.pth\nEpoch 3/25\n  Train Loss: 0.0524, Train Acc: 0.9796\n  Val Loss: 0.1112, Val Acc: 0.9596\nCheckpoint saved: mobilenet_v4_03_0.960.pth\nEpoch 4/25\n  Train Loss: 0.0211, Train Acc: 0.9949\n  Val Loss: 0.1016, Val Acc: 0.9697\nCheckpoint saved: mobilenet_v4_04_0.970.pth\nEpoch 5/25\n  Train Loss: 0.0355, Train Acc: 0.9923\n  Val Loss: 0.1048, Val Acc: 0.9697\n⚠️ No improvement for 1 epoch(s)\nEpoch 6/25\n  Train Loss: 0.0093, Train Acc: 0.9949\n  Val Loss: 0.1099, Val Acc: 0.9697\n⚠️ No improvement for 2 epoch(s)\nEpoch 7/25\n  Train Loss: 0.0139, Train Acc: 0.9974\n  Val Loss: 0.1261, Val Acc: 0.9697\n⚠️ No improvement for 3 epoch(s)\nEpoch 8/25\n  Train Loss: 0.0204, Train Acc: 0.9974\n  Val Loss: 0.1127, Val Acc: 0.9798\nCheckpoint saved: mobilenet_v4_08_0.980.pth\nEpoch 9/25\n  Train Loss: 0.0056, Train Acc: 0.9974\n  Val Loss: 0.1110, Val Acc: 0.9697\n⚠️ No improvement for 1 epoch(s)\nEpoch 10/25\n  Train Loss: 0.0040, Train Acc: 1.0000\n  Val Loss: 0.1246, Val Acc: 0.9596\n⚠️ No improvement for 2 epoch(s)\nEpoch 11/25\n  Train Loss: 0.0038, Train Acc: 1.0000\n  Val Loss: 0.1191, Val Acc: 0.9798\n⚠️ No improvement for 3 epoch(s)\nEpoch 12/25\n  Train Loss: 0.0274, Train Acc: 0.9974\n  Val Loss: 0.1175, Val Acc: 0.9798\n⚠️ No improvement for 4 epoch(s)\nEpoch 13/25\n  Train Loss: 0.0322, Train Acc: 0.9923\n  Val Loss: 0.1488, Val Acc: 0.9495\n⚠️ No improvement for 5 epoch(s)\n🛑 Early stopping triggered after 13 epochs\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Observation:\nPhotometric augmentation (ColorJitter) was evaluated but did not yield additional gains and will be excluded from the final model.\n\nIn these data augmentation experiments that were conducted incrementally,\nGeometric augmentations (rotation, affine transforms) and mild Gaussian blur improved validation performance.\n","metadata":{}},{"cell_type":"markdown","source":"### Data Augmentation: Random Affine\nRandom affine transformation the input keeping center invariant.\n\nParameters:\n\n```degrees``` (sequence or number) – Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). Set to 0 to deactivate rotations.\n\n```translate``` (tuple, optional) – tuple of maximum absolute fraction for horizontal and vertical translations. For example translate=(a, b), then horizontal shift is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n\n```scale``` (tuple, optional) – scaling factor interval, e.g (a, b), then scale is randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n\n\nAlso, here we will reorder the transformations by bringing the Random Affine before Gaussian Blur so that we have geometry first (preserve structure) and blur last (simulates acquisition).","metadata":{}},{"cell_type":"code","source":"input_size = 224\n\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Training transforms WITH augmentation \ntrain_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n\n    transforms.RandomRotation(10),\n\n    transforms.RandomAffine(\n    degrees=3,\n    translate=(0.01, 0.01),\n    scale=(0.99, 1.01)\n    ),\n\n    transforms.GaussianBlur(\n    kernel_size=3,\n    sigma=(0.1, 1.0)\n    ),\n\n    # transforms.RandomHorizontalFlip(p=0.3),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:19:53.100910Z","iopub.execute_input":"2026-01-01T20:19:53.101260Z","iopub.status.idle":"2026-01-01T20:19:53.107742Z","shell.execute_reply.started":"2026-01-01T20:19:53.101225Z","shell.execute_reply":"2026-01-01T20:19:53.106951Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"droprate = 0.2\nsize_inner = 500\nlearning_rate = 1e-4 # 0.001\nnum_epochs = 25\n\n\ntrain_loader, val_loader, test_loader = loader_func(train_dir, test_dir, train_transforms, val_transforms, test_transforms)\n\nprint(f'\\n=== With Data Augmentation using Random Rotation, Random Affine and Gaussian Blur: ===')\nmodel, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner,droprate=droprate)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:08:24.608997Z","iopub.execute_input":"2026-01-01T20:08:24.609882Z","iopub.status.idle":"2026-01-01T20:11:06.175824Z","shell.execute_reply.started":"2026-01-01T20:08:24.609847Z","shell.execute_reply":"2026-01-01T20:11:06.174915Z"}},"outputs":[{"name":"stdout","text":"\n=== With Data Augmentation using Random Rotation, Random Affine and Gaussian Blur: ===\nEpoch 1/25\n  Train Loss: 0.4068, Train Acc: 0.8393\n  Val Loss: 0.2066, Val Acc: 0.8485\nCheckpoint saved: mobilenet_v4_01_0.848.pth\nEpoch 2/25\n  Train Loss: 0.1155, Train Acc: 0.9719\n  Val Loss: 0.1433, Val Acc: 0.9293\nCheckpoint saved: mobilenet_v4_02_0.929.pth\nEpoch 3/25\n  Train Loss: 0.0618, Train Acc: 0.9770\n  Val Loss: 0.1660, Val Acc: 0.9192\n⚠️ No improvement for 1 epoch(s)\nEpoch 4/25\n  Train Loss: 0.0251, Train Acc: 0.9923\n  Val Loss: 0.1065, Val Acc: 0.9798\nCheckpoint saved: mobilenet_v4_04_0.980.pth\nEpoch 5/25\n  Train Loss: 0.0280, Train Acc: 0.9898\n  Val Loss: 0.1025, Val Acc: 0.9697\n⚠️ No improvement for 1 epoch(s)\nEpoch 6/25\n  Train Loss: 0.0052, Train Acc: 1.0000\n  Val Loss: 0.1071, Val Acc: 0.9697\n⚠️ No improvement for 2 epoch(s)\nEpoch 7/25\n  Train Loss: 0.0050, Train Acc: 1.0000\n  Val Loss: 0.1088, Val Acc: 0.9697\n⚠️ No improvement for 3 epoch(s)\nEpoch 8/25\n  Train Loss: 0.0060, Train Acc: 0.9974\n  Val Loss: 0.1210, Val Acc: 0.9798\n⚠️ No improvement for 4 epoch(s)\nEpoch 9/25\n  Train Loss: 0.0036, Train Acc: 1.0000\n  Val Loss: 0.1318, Val Acc: 0.9798\n⚠️ No improvement for 5 epoch(s)\n🛑 Early stopping triggered after 9 epochs\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"#### ⚠️ Observation:\nRandom rotation and photometric augmentations improved generalisation, while additional affine transformations did not yield further gains, indicating limited spatial variance in the dataset.\n\nThe validation accuracy stayed around 98% - although it reached it in earlier epochs as compared to the earlier augmentations, there was no improvement in the score. Therefore we will remove the RandomAffine data augmentation from our transformation. ","metadata":{}},{"cell_type":"markdown","source":"#### Data Augmentation: Horizontal Flip\nHorizontally flip the input with a given probability.\n\nParamters:\n\n``p`` (float, optional) – probability of the input being flipped. Default value is 0.5","metadata":{}},{"cell_type":"code","source":"input_size = 224\n\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Training transforms WITH augmentation \ntrain_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n\n    transforms.RandomRotation(10),\n\n    transforms.GaussianBlur(\n    kernel_size=3,\n    sigma=(0.1, 1.0)\n    ),\n\n    transforms.RandomHorizontalFlip(p=0.3),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:25:27.263208Z","iopub.execute_input":"2026-01-01T20:25:27.263555Z","iopub.status.idle":"2026-01-01T20:25:27.269855Z","shell.execute_reply.started":"2026-01-01T20:25:27.263526Z","shell.execute_reply":"2026-01-01T20:25:27.269216Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"droprate = 0.2\nsize_inner = 500\nlearning_rate = 1e-4 # 0.001\nnum_epochs = 25\n\n\ntrain_loader, val_loader, test_loader = loader_func(train_dir, test_dir, train_transforms, val_transforms, test_transforms)\n\nprint(f'\\n=== With Data Augmentation using Random Rotation, Gaussian Blur and Horizontal Flip: ===')\nmodel, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner,droprate=droprate)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:26:08.193492Z","iopub.execute_input":"2026-01-01T20:26:08.194222Z","iopub.status.idle":"2026-01-01T20:29:01.478248Z","shell.execute_reply.started":"2026-01-01T20:26:08.194189Z","shell.execute_reply":"2026-01-01T20:29:01.477478Z"}},"outputs":[{"name":"stdout","text":"\n=== With Data Augmentation using Random Rotation, Gaussian Blur and Horizontal Flip: ===\nEpoch 1/25\n  Train Loss: 0.4552, Train Acc: 0.8010\n  Val Loss: 0.2643, Val Acc: 0.8485\nCheckpoint saved: mobilenet_v4_01_0.848.pth\nEpoch 2/25\n  Train Loss: 0.1530, Train Acc: 0.9413\n  Val Loss: 0.1461, Val Acc: 0.9293\nCheckpoint saved: mobilenet_v4_02_0.929.pth\nEpoch 3/25\n  Train Loss: 0.0542, Train Acc: 0.9898\n  Val Loss: 0.1006, Val Acc: 0.9495\nCheckpoint saved: mobilenet_v4_03_0.949.pth\nEpoch 4/25\n  Train Loss: 0.0351, Train Acc: 0.9821\n  Val Loss: 0.1076, Val Acc: 0.9596\nCheckpoint saved: mobilenet_v4_04_0.960.pth\nEpoch 5/25\n  Train Loss: 0.0157, Train Acc: 0.9974\n  Val Loss: 0.1082, Val Acc: 0.9697\nCheckpoint saved: mobilenet_v4_05_0.970.pth\nEpoch 6/25\n  Train Loss: 0.0733, Train Acc: 0.9898\n  Val Loss: 0.1134, Val Acc: 0.9596\n⚠️ No improvement for 1 epoch(s)\nEpoch 7/25\n  Train Loss: 0.0889, Train Acc: 0.9770\n  Val Loss: 0.1148, Val Acc: 0.9596\n⚠️ No improvement for 2 epoch(s)\nEpoch 8/25\n  Train Loss: 0.0435, Train Acc: 0.9847\n  Val Loss: 0.1204, Val Acc: 0.9596\n⚠️ No improvement for 3 epoch(s)\nEpoch 9/25\n  Train Loss: 0.0133, Train Acc: 1.0000\n  Val Loss: 0.1100, Val Acc: 0.9596\n⚠️ No improvement for 4 epoch(s)\nEpoch 10/25\n  Train Loss: 0.0223, Train Acc: 0.9923\n  Val Loss: 0.1022, Val Acc: 0.9596\n⚠️ No improvement for 5 epoch(s)\n🛑 Early stopping triggered after 10 epochs\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"#### ⚠️ Observation:\nThe horizontal flip actually hurts the validation accuracy brining it down to 97% from 98%. Therefore we decide to remove the transformation from our pipeline.\n\nWhy horizontal flip doesn’t help for cataract detection: This is actually expected for ophthalmology data. Left–right anatomy matters- Fundus images still contain directional structure and optic disc, vessels, illumination patterns differ left vs right.","metadata":{}},{"cell_type":"markdown","source":"### Final Data Augmentation Pipeline:","metadata":{}},{"cell_type":"markdown","source":"### ✅ Final Augmentation Pipeline Structure:\n\n- Resize (224 × 224)\nEnsures compatibility with MobileNetV2 and preserves pretraining assumptions from ImageNet.\n\n- RandomRotation (±10°)\nImproves robustness to small camera and eye orientation variations commonly present in real-world ophthalmic images.\n\n- Gaussian Blur (mild)\nSimulates variations in image focus and acquisition quality, which are frequent in clinical settings.\n\n- Normalization (ImageNet statistics)\nAligns input distribution with the pretrained backbone, enabling stable and effective transfer learning.\n\n- No augmentation on validation/test sets\nGuarantees unbiased evaluation by measuring performance on unaltered data.","metadata":{}},{"cell_type":"markdown","source":"### 🚫 Explicitly Excluded Augmentations\n\nThe following were evaluated but excluded from the final pipeline:\n\nColorJitter (photometric augmentation)\nDid not yield consistent validation improvements and risked altering clinically meaningful color cues.\n\nRandomAffine & HorizontalFlip\nProvided limited or no gains and were omitted to avoid unnecessary transformation complexity.","metadata":{}},{"cell_type":"code","source":"input_size = 224\n\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Training transforms WITH augmentation \ntrain_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n\n    transforms.RandomRotation(10),\n\n    transforms.GaussianBlur(\n    kernel_size=3,\n    sigma=(0.1, 1.0)\n    ),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:48:40.242422Z","iopub.execute_input":"2026-01-01T20:48:40.243296Z","iopub.status.idle":"2026-01-01T20:48:40.249169Z","shell.execute_reply.started":"2026-01-01T20:48:40.243262Z","shell.execute_reply":"2026-01-01T20:48:40.248331Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"droprate = 0.2\nsize_inner = 500\nlearning_rate = 1e-4 # 0.001\nnum_epochs = 25\n\n\ntrain_loader, val_loader, test_loader = loader_func(train_dir, test_dir, train_transforms, val_transforms, test_transforms)\n\nprint(f'\\n=== With Data Augmentation with the Final Pipeline using Random Rotation and Gaussian Blur : ===')\nmodel, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner,droprate=droprate)\ntrain_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:49:19.660698Z","iopub.execute_input":"2026-01-01T20:49:19.661005Z","iopub.status.idle":"2026-01-01T20:52:28.811260Z","shell.execute_reply.started":"2026-01-01T20:49:19.660979Z","shell.execute_reply":"2026-01-01T20:52:28.810473Z"}},"outputs":[{"name":"stdout","text":"\n=== With Data Augmentation with the Final Pipeline using Random Rotation and Gaussian Blur : ===\nEpoch 1/25\n  Train Loss: 0.4176, Train Acc: 0.8342\n  Val Loss: 0.2234, Val Acc: 0.8586\nCheckpoint saved: mobilenet_v4_01_0.859.pth\nEpoch 2/25\n  Train Loss: 0.1370, Train Acc: 0.9490\n  Val Loss: 0.1594, Val Acc: 0.9192\nCheckpoint saved: mobilenet_v4_02_0.919.pth\nEpoch 3/25\n  Train Loss: 0.0460, Train Acc: 0.9847\n  Val Loss: 0.1060, Val Acc: 0.9697\nCheckpoint saved: mobilenet_v4_03_0.970.pth\nEpoch 4/25\n  Train Loss: 0.0444, Train Acc: 0.9872\n  Val Loss: 0.0888, Val Acc: 0.9697\n⚠️ No improvement for 1 epoch(s)\nEpoch 5/25\n  Train Loss: 0.0120, Train Acc: 1.0000\n  Val Loss: 0.0959, Val Acc: 0.9596\n⚠️ No improvement for 2 epoch(s)\nEpoch 6/25\n  Train Loss: 0.0130, Train Acc: 1.0000\n  Val Loss: 0.0997, Val Acc: 0.9798\nCheckpoint saved: mobilenet_v4_06_0.980.pth\nEpoch 7/25\n  Train Loss: 0.0080, Train Acc: 0.9974\n  Val Loss: 0.1131, Val Acc: 0.9798\n⚠️ No improvement for 1 epoch(s)\nEpoch 8/25\n  Train Loss: 0.0032, Train Acc: 1.0000\n  Val Loss: 0.1171, Val Acc: 0.9697\n⚠️ No improvement for 2 epoch(s)\nEpoch 9/25\n  Train Loss: 0.0022, Train Acc: 1.0000\n  Val Loss: 0.1152, Val Acc: 0.9798\n⚠️ No improvement for 3 epoch(s)\nEpoch 10/25\n  Train Loss: 0.0011, Train Acc: 1.0000\n  Val Loss: 0.1121, Val Acc: 0.9798\n⚠️ No improvement for 4 epoch(s)\nEpoch 11/25\n  Train Loss: 0.0009, Train Acc: 1.0000\n  Val Loss: 0.1134, Val Acc: 0.9798\n⚠️ No improvement for 5 epoch(s)\n🛑 Early stopping triggered after 11 epochs\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### Testing on the Test set","metadata":{}},{"cell_type":"markdown","source":"Using the same model as above and not recreating model architecture","metadata":{}},{"cell_type":"code","source":"input_size = 224\n\n# ImageNet normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Training transforms WITH augmentation \ntrain_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n\n    transforms.RandomRotation(10),\n\n    transforms.GaussianBlur(\n    kernel_size=3,\n    sigma=(0.1, 1.0)\n    ),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((input_size, input_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:40:55.194081Z","iopub.execute_input":"2026-01-01T20:40:55.194944Z","iopub.status.idle":"2026-01-01T20:40:55.201025Z","shell.execute_reply.started":"2026-01-01T20:40:55.194899Z","shell.execute_reply":"2026-01-01T20:40:55.200295Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = CataractDetectorMobileNet(num_classes=2)\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:56:38.826286Z","iopub.execute_input":"2026-01-01T20:56:38.826902Z","iopub.status.idle":"2026-01-01T20:56:38.933200Z","shell.execute_reply.started":"2026-01-01T20:56:38.826869Z","shell.execute_reply":"2026-01-01T20:56:38.932663Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"Loading the checkpoint weights","metadata":{}},{"cell_type":"code","source":"checkpoint_path = \"mobilenet_v4_06_0.980.pth\"\nmodel.load_state_dict(torch.load(checkpoint_path, map_location=device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:56:44.310007Z","iopub.execute_input":"2026-01-01T20:56:44.310600Z","iopub.status.idle":"2026-01-01T20:56:44.392769Z","shell.execute_reply.started":"2026-01-01T20:56:44.310570Z","shell.execute_reply":"2026-01-01T20:56:44.392168Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:56:46.854698Z","iopub.execute_input":"2026-01-01T20:56:46.854978Z","iopub.status.idle":"2026-01-01T20:56:46.862364Z","shell.execute_reply.started":"2026-01-01T20:56:46.854955Z","shell.execute_reply":"2026-01-01T20:56:46.861770Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"CataractDetectorMobileNet(\n  (base_model): MobileNetV2(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n    )\n    (classifier): Identity()\n  )\n  (global_avg_pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n  (inner): Linear(in_features=1280, out_features=500, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.2, inplace=False)\n  (output_layer): Linear(in_features=500, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"Runing inference on the test set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():  # no gradients needed\n    for inputs, labels in test_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:56:53.662071Z","iopub.execute_input":"2026-01-01T20:56:53.662601Z","iopub.status.idle":"2026-01-01T20:56:59.393027Z","shell.execute_reply.started":"2026-01-01T20:56:53.662572Z","shell.execute_reply":"2026-01-01T20:56:59.392257Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"### Compute test metrics","metadata":{}},{"cell_type":"markdown","source":"#### Accuracy","metadata":{}},{"cell_type":"code","source":"test_acc = accuracy_score(all_labels, all_preds)\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:57:07.444601Z","iopub.execute_input":"2026-01-01T20:57:07.445375Z","iopub.status.idle":"2026-01-01T20:57:07.453380Z","shell.execute_reply.started":"2026-01-01T20:57:07.445318Z","shell.execute_reply":"2026-01-01T20:57:07.452783Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.9835\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"#### Classification Report","metadata":{}},{"cell_type":"code","source":"print(classification_report(\n    all_labels,\n    all_preds,\n    target_names=[\"Normal\", \"Cataract\"]\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:57:10.221478Z","iopub.execute_input":"2026-01-01T20:57:10.222209Z","iopub.status.idle":"2026-01-01T20:57:10.234984Z","shell.execute_reply.started":"2026-01-01T20:57:10.222180Z","shell.execute_reply":"2026-01-01T20:57:10.234270Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n      Normal       1.00      0.97      0.98        61\n    Cataract       0.97      1.00      0.98        60\n\n    accuracy                           0.98       121\n   macro avg       0.98      0.98      0.98       121\nweighted avg       0.98      0.98      0.98       121\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"#### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(all_labels, all_preds)\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:57:13.369560Z","iopub.execute_input":"2026-01-01T20:57:13.370055Z","iopub.status.idle":"2026-01-01T20:57:13.376668Z","shell.execute_reply.started":"2026-01-01T20:57:13.370024Z","shell.execute_reply":"2026-01-01T20:57:13.376029Z"}},"outputs":[{"name":"stdout","text":"[[59  2]\n [ 0 60]]\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### 🚀 Observations from Testing\n\nThe final MobileNetV2-based cataract detection model was evaluated on a\nheld-out test set consisting of 121 images. The model achieved a test\naccuracy of **98.35%**, demonstrating strong generalization performance.\n\nThe classification report shows balanced precision, recall, and F1-scores\nof **0.98** across both Normal and Cataract classes. Notably, the model\nachieved **100% recall for cataract cases**, indicating that no cataract\ninstances were missed.\n\nThe confusion matrix further confirms this result, with only two normal\nimages misclassified as cataract and zero false negatives.\n","metadata":{"execution":{"iopub.status.busy":"2026-01-01T20:59:33.010981Z","iopub.execute_input":"2026-01-01T20:59:33.011628Z","iopub.status.idle":"2026-01-01T20:59:33.015284Z","shell.execute_reply.started":"2026-01-01T20:59:33.011598Z","shell.execute_reply":"2026-01-01T20:59:33.014160Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}